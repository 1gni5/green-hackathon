{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from examples.serializer import Serializer\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = 'results.csv'  # Update with your CSV file path\n",
    "df = pd.read_csv(csv_file_path, names=Serializer.get_column_names())\n",
    "\n",
    "# Drop columns that are all 0\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "# Multiply the values of last column by 10^6\n",
    "df.iloc[:, -1] = df.iloc[:, -1].apply(lambda x: x * 1000000)\n",
    "\n",
    "\n",
    "# Assuming the last column is the target variable, and the rest are features\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8, 2499,    3],\n",
       "       [  16, 2499,    3],\n",
       "       [  32, 2499,    3],\n",
       "       [  64, 2499,    3],\n",
       "       [   8, 2499,    3],\n",
       "       [  16, 2499,    3],\n",
       "       [  32, 2499,    3],\n",
       "       [  64, 2499,    3],\n",
       "       [   8, 2499,    3],\n",
       "       [  16, 2499,    3],\n",
       "       [  32, 2499,    3],\n",
       "       [  64, 2499,    3],\n",
       "       [   8, 2499,    3],\n",
       "       [  16, 2499,    3],\n",
       "       [  32, 2499,    3],\n",
       "       [  64, 2499,    3],\n",
       "       [   8, 2499,    3],\n",
       "       [  16, 2499,    3],\n",
       "       [  32, 2499,    3],\n",
       "       [  64, 2499,    3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the DNN model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.6824 - accuracy: 0.0000e+00 - val_loss: 0.6683 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/25\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6775 - accuracy: 0.0000e+00 - val_loss: 0.6614 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/25\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6726 - accuracy: 0.0000e+00 - val_loss: 0.6549 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/25\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6680 - accuracy: 0.0000e+00 - val_loss: 0.6487 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/25\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6635 - accuracy: 0.0000e+00 - val_loss: 0.6427 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/25\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6591 - accuracy: 0.0000e+00 - val_loss: 0.6368 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/25\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6548 - accuracy: 0.0000e+00 - val_loss: 0.6309 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/25\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6506 - accuracy: 0.0000e+00 - val_loss: 0.6251 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/25\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6465 - accuracy: 0.0000e+00 - val_loss: 0.6193 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/25\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6424 - accuracy: 0.0000e+00 - val_loss: 0.6137 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/25\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6382 - accuracy: 0.0000e+00 - val_loss: 0.6081 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/25\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6341 - accuracy: 0.0000e+00 - val_loss: 0.6026 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/25\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6299 - accuracy: 0.0000e+00 - val_loss: 0.5972 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/25\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6258 - accuracy: 0.0000e+00 - val_loss: 0.5921 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/25\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6217 - accuracy: 0.0000e+00 - val_loss: 0.5871 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/25\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6177 - accuracy: 0.0000e+00 - val_loss: 0.5820 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/25\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6137 - accuracy: 0.0000e+00 - val_loss: 0.5770 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/25\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6096 - accuracy: 0.0000e+00 - val_loss: 0.5719 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/25\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6056 - accuracy: 0.0000e+00 - val_loss: 0.5669 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/25\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6016 - accuracy: 0.0000e+00 - val_loss: 0.5616 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/25\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5976 - accuracy: 0.0000e+00 - val_loss: 0.5563 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/25\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5936 - accuracy: 0.0000e+00 - val_loss: 0.5511 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/25\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5895 - accuracy: 0.0000e+00 - val_loss: 0.5458 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/25\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5855 - accuracy: 0.0000e+00 - val_loss: 0.5406 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/25\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5814 - accuracy: 0.0000e+00 - val_loss: 0.5352 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28fc85d30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=25, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5808 - accuracy: 0.0000e+00\n",
      "Test Loss: 0.5808389782905579, Test Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x28fc9cdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predictions: [[0.45875785]\n",
      " [0.43850186]\n",
      " [0.33445185]\n",
      " [0.43850186]]\n",
      "Actual: [0.08687134 0.14555456 0.11406313 0.07761291]\n"
     ]
    }
   ],
   "source": [
    "# Compare the predictions with the actual values\n",
    "predictions = model.predict(X_test)\n",
    "print(f'Predictions: {predictions}')\n",
    "print(f'Actual: {y_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
